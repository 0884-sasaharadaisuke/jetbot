{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following \n",
    "\n",
    "If you've run through the collision avoidance sample, your should be familiar following three steps\n",
    "\n",
    "(訳) collision avoidanceを実行していれば、今回のこの3ステップには馴染みがあるでしょう。\n",
    "\n",
    "1.  Data collection\n",
    "2.  Training\n",
    "3.  Deployment\n",
    "\n",
    "(訳)\n",
    "1. Data収集\n",
    "2. 学習\n",
    "3. デプロイ\n",
    "\n",
    "In this notebook, we'll do the same exact thing!  Except, instead of classification, you'll learn a different fundamental technique, **regression**, that we'll use to\n",
    "enable JetBot to follow a road (or really, any path or target point).  \n",
    "\n",
    "(訳) このnotebookは、同じことをします。ただし、今回はJetBotが道路（または実際に任意のパスまたはターゲットポイント）をたどることができるようにするために、画像分類の代わりに、**回帰**という別の基本的な手法で学習します。\n",
    "\n",
    "1. Place the JetBot in different positions on a path (offset from center, different angles, etc)\n",
    "\n",
    "(訳) 1. JetBotをパス上のさまざまな位置に配置します（中心からのオフセット、さまざまな角度など）\n",
    "\n",
    ">  Remember from collision avoidance, data variation is key!\n",
    "\n",
    "(訳) データのバリデーションこそが重要であると、collision avoidanceのときに覚えたはずです。\n",
    "\n",
    "2. Display the live camera feed from the robot\n",
    "3. Using a gamepad controller, place a 'green dot', which corresponds to the target direction we want the robot to travel, on the image.\n",
    "4. Store the X, Y values of this green dot along with the image from the robot's camera\n",
    "\n",
    "(訳)\n",
    "2. robotからのライブカメラフィードを画面に表示します\n",
    "3. ゲームパッドを使い(改良して本サンプルはマウスのみ)、`greenの点`を画像上でロボットが移動するターゲットの方向に移動します。\n",
    "4. この`greenの点`のX、Y値をロボットのカメラからの画像と、一緒に保存します\n",
    "\n",
    "Then, in the training notebook, we'll train a neural network to predict the X, Y values of our label.  In the live demo, we'll use\n",
    "the predicted X, Y values to compute an approximate steering value (it's not 'exactly' an angle, as\n",
    "that would require image calibration, but it's roughly proportional to the angle so our controller will work fine).\n",
    "\n",
    "(訳)\n",
    "notebookの学習で、ラベルのX, Yの値の予測をおこなうためにneural networkで学習します。このライブデモでは、概算のステアリング値を計算するために予測されたX、Y値を使用します。(角度として「正確に」ではありません。画像のキャリブレーションが必要になりますが、角度にほぼ比例するため、制御は正常に機能します)\n",
    "\n",
    "So how do you decide exactly where to place the target for this example?  Here is a guide we think may help\n",
    "\n",
    "(訳)\n",
    "それでは、この例の目標点は、正確にどこに配置するのがいいでしょうか？ ここに役立つと思われるガイドがあります。\n",
    "\n",
    "1.  Look at the live video feed from the camera\n",
    "2.  Imagine the path that the robot should follow (try to approximate the distance it needs to avoid running off road etc.)\n",
    "3.  Place the target as far along this path as it can go so that the robot could head straight to the target without 'running off' the road.\n",
    "\n",
    "(訳)\n",
    "1. カメラからのライブデモフィードを見ます\n",
    "2. ロボットがたどるべき経路を想像してください（道路からの脱出などを回避するために必要な距離を概算してみてください）\n",
    "3. ロボットが道路を「脱線」する事なく目標点にまっすぐ進むことができるように、目標点はこの経路に沿ってできるだけ遠くに配置します。\n",
    "\n",
    "> For example, if we're on a very straight road, we could place it at the horizon.  If we're on a sharp turn, it may need to be placed closer to the robot so it doesn't run out of boundaries.\n",
    "\n",
    "(訳)\n",
    "> 例として、もし真っ直ぐのロードがあったとします、目標点は地平線にします。もし、鋭く曲がったカーブなら、脱線しないレベルで、ロボットの近くに配置する必要があるでしょう。\n",
    "\n",
    "Assuming our deep learning model works as intended, these labeling guidelines should ensure the following:\n",
    "\n",
    "(訳) \n",
    "ディープラーニングのモデルが意図したとおりに機能すると仮定すると、これらのラベリングガイドラインでは次のことが保証されます。\n",
    "\n",
    "1.  The robot can safely travel directly towards the target (without going out of bounds etc.)\n",
    "2.  The target will continuously progress along our imagined path\n",
    "\n",
    "(訳)\n",
    "1.ロボットは、ターゲットに向かって安全に直接移動できます（範囲外に出ることなく）。\n",
    "2.目標点は、想像した経路に沿って継続的に処理されます。\n",
    "\n",
    "What we get, is a 'carrot on a stick' that moves along our desired trajectory.  Deep learning decides where to place the carrot, and JetBot just follows it :)\n",
    "\n",
    "(訳)\n",
    "この処理で得られるものは、望む軌道に沿って動く「スティック上のニンジン」です。ディープラーニングがニンジンを配置する場所を決定し、JetBotがそれに追従します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling example video(ラベリングのデモ動画)\n",
    "\n",
    "Execute the block of code to see an example of how to we labeled the images.  This model worked after only 123 images :)\n",
    "\n",
    "(訳) \n",
    "下記のコードを実行して、画像にラベルを付ける方法の例を確認します。このモデルはたった123枚の画像で動作しました:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FW4En6LejhI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FW4En6LejhI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries(ライブラリのimport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets get started by importing all the required libraries for \"data collection\" purpose. We will mainly use OpenCV to visualize and save image with labels. Libraries such as uuid, datetime are used for image naming. \n",
    "\n",
    "(訳)それでは、\"data collection\"のために必要なライブラリ全部をimportして、作業を開始します。ラベルと紐付いた画像を表示し、保存するためにOpenCVを使用します。uuid、datetimeなどのライブラリは、イメージのファイル名の命名に使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Python basic pakcages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Live Camera Feed(ライブカメラフィードの表示)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's initialize and display our camera like we did in the teleoperation notebook. \n",
    "\n",
    "(訳)　最初に、teleperation notebookで実行したようにカメラの初期化と表示をおこないます。\n",
    "\n",
    "We use Camera Class from JetBot to enable CSI MIPI camera. Our neural network takes a 224x224 pixel image as input. We'll set our camera to that size to minimize the filesize of our dataset (we've tested that it works for this task). In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later.\n",
    "\n",
    "(訳) JetBotのカメラクラスは、CSI MIPI cameraを有効にするために使います。neural networkでは、224x224ピクセスの画像データをインプットとして使います。データセットのファイルサイズを最小化するために、カメラをそのサイズに設定します(このタスクのために動作する動作する事は確認しています) いくつかのシナリオでは、画像サイズを大きくし収集し、後で目的のサイズに縮小する方がいいかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a5906a29584d03bab5480061861613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a5c8a8dce34bf48052606e51d420fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='x', max=1.0, min=-1.0, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea89dffa232f45caadb060c3a8c97f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatSlider(value=0.0, description='y', max=1.0, min=-1.0, step=0.001)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "camera = Camera()\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "target_widget = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "x_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='x')\n",
    "y_slider = widgets.FloatSlider(min=-1.0, max=1.0, step=0.001, description='y')\n",
    "\n",
    "def display_xy(camera_image):\n",
    "    image = np.copy(camera_image)\n",
    "    x = x_slider.value\n",
    "    y = y_slider.value\n",
    "    x = int(x * 224 / 2 + 112)\n",
    "    y = int(y * 224 / 2 + 112)\n",
    "    image = cv2.circle(image, (x, y), 8, (0, 255, 0), 3)\n",
    "    image = cv2.circle(image, (112, 224), 8, (0, 0,255), 3)\n",
    "    image = cv2.line(image, (x,y), (112,224), (255,0,0), 3)\n",
    "    jpeg_image = bgr8_to_jpeg(image)\n",
    "    return jpeg_image\n",
    "\n",
    "time.sleep(1)\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "traitlets.dlink((camera, 'value'), (target_widget, 'value'), transform=display_xy)\n",
    "\n",
    "display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data(データ収集)\n",
    "\n",
    "The following block of code will display the live image feed, as well as the number of images we've saved.  We store\n",
    "the target X, Y values by\n",
    "\n",
    "(訳) 次のコードブロックは、ライブ画像フィードと保存した画像の数を表示します。ターゲットとなるX、Y値の値を下記のように保存します。\n",
    "\n",
    "1. Place the green dot on the target\n",
    "2. Press 'down' on the DPAD to save\n",
    "\n",
    "(訳)\n",
    "1. 目標点に緑のドットを移動します。\n",
    "2. 保存ボタンを押します(本サンプル用に、オフィシャルのサンプルを修正しています。)\n",
    "\n",
    "This will store a file in the ``dataset_xy`` folder with files named\n",
    "\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "(訳)\n",
    "これで、``dataset_xy``フォルダーに、\n",
    "\n",
    "``xy_<x value>_<y value>_<uuid>.jpg``\n",
    "\n",
    "の形式で保存されます。\n",
    "\n",
    "When we train, we load the images and parse the x, y values from the filename\n",
    "\n",
    "(訳)\n",
    "学習時に、ファイル名からx,yの値をパースし、イメージとともに読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f73c8e7a7e4551baef6c38cf9b392b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_DIR = 'dataset_xy'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(DATASET_DIR)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')\n",
    "\n",
    "#for b in controller.buttons:\n",
    "#    b.unobserve_all()\n",
    "\n",
    "count_widget = widgets.IntText(description='count', value=len(glob.glob(os.path.join(DATASET_DIR, '*.jpg'))))\n",
    "\n",
    "def xy_uuid(x, y):\n",
    "    return 'xy_%03d_%03d_%s' % (x * 50 + 50, y * 50 + 50, uuid1())\n",
    "\n",
    "def save_snapshot():\n",
    "    #if change['new']:\n",
    "    uuid = xy_uuid(x_slider.value, y_slider.value)\n",
    "    image_path = os.path.join(DATASET_DIR, uuid + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image_widget.value)\n",
    "        count_widget.value = len(glob.glob(os.path.join(DATASET_DIR, '*.jpg')))\n",
    "\n",
    "#controller.buttons[13].observe(save_snapshot, names='value')\n",
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "add_button = widgets.Button(description='add', button_style='success', layout=button_layout)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    target_widget,\n",
    "    x_slider,\n",
    "    y_slider,\n",
    "    count_widget,\n",
    "    add_button\n",
    "]))\n",
    "\n",
    "add_button.on_click(lambda x: save_snapshot())\n",
    "\n",
    "#display(widgets.HBox([image_widget, target_widget]), x_slider, y_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next(次)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
