{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Following - Live demo(デモ:TensorRT対応)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use model we trained to move jetBot smoothly on track. \n",
    "\n",
    "このノートブックでは、トラックの上をスムーズにJetBotが移動させるために学習済みモデルを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model(学習済みモデルの読み込み)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that you have already downloaded ``best_steering_model_xy.pth`` to work station as instructed in \"train_model.ipynb\" notebook. Now, you should upload model file to JetBot in to this notebooks's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` in this notebook's directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please make sure the file has uploaded fully before calling the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook.\n",
    "\n",
    "(訳) PyTorchモデルを初期化するために下記コードを実行します。これはトレーニング用のnotebookでもおなじみですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_steering_model_xy.pth`` file that you uploaded.\n",
    "\n",
    "(訳) 次に、``best_steering_model_xy.pth``から学習の重みづけをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best_steering_model_xy.pth'))\n",
    "\n",
    "# TensorRT対応の改造\n",
    "from torch2trt import TRTModule \n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load('road_following_model_xy_trt.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device.\n",
    "\n",
    "(訳) 現在、モデルの重みはCPUメモリにあり、以下のコードを実行してGPUデバイスに転送します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "#model = model.to(device)\n",
    "#model = model.eval().half()\n",
    "\n",
    "# TensorRT対応の改造\n",
    "model = model_trt.to(device)\n",
    "model = model_trt.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our model, but there's a slight issue. The format that we trained our model doesnt exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "(訳) モデルを読み込みましたが、まだわずかな問題があります。学習済みモデルのフォーマットと、カメラの形式と完全に一致しません。これを解消するために、 いくつかの前処理を行う必要があります。これらは、下記の手順になります。\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension\n",
    "\n",
    "(訳)\n",
    "1. HWC layoutからCHW layoutに変換\n",
    "2. トレーニング中に使ったのと同じパラメーターを使用して正規化します（カメラは[0、255]の範囲の値を提供し、ロードされた画像は[0、1]の範囲でトレーニングするため、255.0でスケーリングする必要があります\n",
    "3. dataをCPUメモリからGPUメモリに転送します\n",
    "4. バッチディメンションを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "(訳) すばらしい、これでカメラ形式からneural networkのインプットのフォーマットに変換するための、pre-processing関数を定義する事ができました。　\n",
    "\n",
    "Now, let's start and display our camera. You should be pretty familiar with this by now. \n",
    "\n",
    "(訳) カメラを起動し、画面に表示します。カメラの起動には、大分なれてきましたね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera()\n",
    "\n",
    "image_widget = ipywidgets.Image()\n",
    "\n",
    "traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors.\n",
    "\n",
    "(訳)モーターの制御に必要なrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define sliders to control JetBot\n",
    "> Note: We have initialize the slider values for best known configurations, however these might not work for your dataset, therefore please increase or decrease the sliders according to your setup and environment\n",
    "\n",
    "(訳) JetBotを制御するためにsliderを制御します。\n",
    "> メモ: 最もよく知られている構成のために、スライダー値を初期化していますが、あなたのデータセットでは、うまく機能しない可能性があるります。そのため、環境に応じてスライダーを増減してセットアップしてください。\n",
    "\n",
    "1. Speed Control (speed_gain_slider): To start your JetBot increase ``speed_gain_slider`` \n",
    "2. Steering Gain Control (steering_gain_sloder): If you see JetBot is woblling, you need to reduce ``steering_gain_slider`` till it is smooth\n",
    "3. Steering Bias control (steering_bias_slider): If you see JetBot is biased towards extreme right or extreme left side of the track, you should control this slider till JetBot start following line or track in the center.  This accounts for motor biases as well as camera offsets\n",
    "\n",
    "(訳)\n",
    "1. スピードコントローラー(speed_gain_slider): ``speed_gain_slider``の値を増やすとJetBotがスタートします。\n",
    "2. ステアリングゲインコントローラー (steering_gain_sloder):JetBotが左右にブレて不安定な場合は、スムーズに走るようになるまで、`` steering_gain_slider``の値を減らす必要があります\n",
    "3. バイアスコントローラー(steering_bias_slider): JetBotがトラックの右端または左端に偏って走行する場合は、JetBotが中央のラインまたはトラックをたどるまでこのスライダーの値を調整します。ここでは、モーターバイアスと同様にカメラのoffsetも考慮します。\n",
    "\n",
    "> Note: You should play around above mentioned sliders with lower speed to get smooth JetBot road following behavior.\n",
    "\n",
    "(訳)\n",
    "> 注：JetBotの道路追従動作をスムーズにする必要するために、上記のスライダーを低速で移動して調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, description='speed gain')\n",
    "steering_gain_slider = ipywidgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.2, description='steering gain')\n",
    "steering_dgain_slider = ipywidgets.FloatSlider(min=0.0, max=0.5, step=0.001, value=0.0, description='steering kd')\n",
    "steering_bias_slider = ipywidgets.FloatSlider(min=-0.3, max=0.3, step=0.01, value=0.0, description='steering bias')\n",
    "\n",
    "display(speed_gain_slider, steering_gain_slider, steering_dgain_slider, steering_bias_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's display some sliders that will let us see what JetBot is thinking.  The x and y sliders will display the predicted x, y values.\n",
    "\n",
    "(訳) 次に、JetBotが何を考えているかを表示するためにいくつかのsliderを画面に表示します。x, y sliderは、推論されたx, yの値が表示されます。\n",
    "\n",
    "The steering slider will display our estimated steering value.  Please remember, this value isn't the actual angle of the target, but simply a value that is\n",
    "nearly proportional.  When the actual angle is ``0``, this will be zero, and it will increase / decrease with the actual angle.\n",
    "\n",
    "(訳) ステアリングsliderは、推定ステアリング値が表示されます。忘れないでください、この値はターゲットの実際の角度ではなく、ほぼ比例した単純な値です。実際の角度が``0``の場合は、これはゼロを意味し、実際の角度で増減します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='x')\n",
    "y_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='y')\n",
    "steering_slider = ipywidgets.FloatSlider(min=-1.0, max=1.0, description='steering')\n",
    "speed_slider = ipywidgets.FloatSlider(min=0, max=1.0, orientation='vertical', description='speed')\n",
    "\n",
    "display(ipywidgets.HBox([y_slider, speed_slider]))\n",
    "display(x_slider, steering_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "(訳)次は、カメラの画像が変わるたびに呼び出される関数を生成します。この関数は、下記ステップを実行します。\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. Compute the approximate steering value\n",
    "4. Control the motors using proportional / derivative control (PD)\n",
    "\n",
    "(訳)\n",
    "1. カメラのイメージのPre-process\n",
    "2. neural networkの実行\n",
    "3. ステアリング値を計算する\n",
    "4. 比例/微分制御（PD）を使用してモーターを制御する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = 0.0\n",
    "angle_last = 0.0\n",
    "\n",
    "def execute(change):\n",
    "    global angle, angle_last\n",
    "    image = change['new']\n",
    "    xy = model(preprocess(image)).detach().float().cpu().numpy().flatten()\n",
    "    x = xy[0]\n",
    "    y = (0.5 - xy[1]) / 2.0\n",
    "    \n",
    "    x_slider.value = x\n",
    "    y_slider.value = y\n",
    "    \n",
    "    speed_slider.value = speed_gain_slider.value\n",
    "    \n",
    "    angle = np.arctan2(x, y)\n",
    "    pid = angle * steering_gain_slider.value + (angle - angle_last) * steering_dgain_slider.value\n",
    "    angle_last = angle\n",
    "    \n",
    "    steering_slider.value = pid + steering_bias_slider.value\n",
    "    \n",
    "    robot.left_motor.value = max(min(speed_slider.value + steering_slider.value, 1.0), 0.0)\n",
    "    robot.right_motor.value = max(min(speed_slider.value - steering_slider.value, 1.0), 0.0)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "(訳)素晴らしい! neural network実行関数が作られましたが、カメラをプロセスに紐付けます。\n",
    "\n",
    "We accomplish that with the observe function.\n",
    "\n",
    "(訳) observe関数でそれを実現します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!\n",
    "\n",
    "> (訳)\n",
    "> 注意: このコードでロボットが動きだします。十分なスペースを確保してください。collision avoidanceが動くでしょう、neral networkは、訓練されたデータと同じくらいに動くはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "(訳) 驚くばかり！ロボットが接続されている場合、新しいカメラフレームごとに新しいコマンドが生成されるはずです。\n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\n",
    "\n",
    "(訳) あなたがデータセットを作ったLegoかトラックのの上にJetBotを置いてください、そしてトラックをフォローするかどうか確認してください。\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below.\n",
    "\n",
    "(訳) この挙動を止めたい場合は、下記コードを実行する事で、紐付けをやめれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "追記) うまくモーターが止まらない場合は、Robotクラスを再度初期化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track follwing the road!!!\n",
    "\n",
    "(訳) 以上がLive demoです。うまく行けば、ロボットが道路をスムーズにしたのではないでしょうか？　\n",
    "\n",
    "If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)\n",
    "\n",
    "(訳) avoiding collisionsが上手く行かない場合、失敗した箇所を見つけてください。美しさは、これら上手くいかないシナリオについてより多くのデータを収集すれば、ロボットはさらに良くなるはずです"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
