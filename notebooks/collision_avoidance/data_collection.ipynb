{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance - Data Collection（データの収集)\n",
    "\n",
    "If you ran through the basic motion notebook, hopefully you're enjoying how easy it can be to make your Jetbot move around! Thats very cool!  But what's even cooler, is making JetBot move around all by itself!  \n",
    "\n",
    "[訳] basic motionのノートブックを実行し、Jetbotを簡単に動かすことができ、お楽しみいただけたのではないでしょうか？ それはとてもいい経験です。でも、もっとすごいのは、JetBotは、自律的に動き回る事もできます。\n",
    "\n",
    "This is a super hard task, that has many different approaches but the whole problem is usually broken down into easier sub-problems.  It could be argued that one of the most\n",
    "important sub-problems to solve, is the problem of preventing the robot from entering dangerous situations!  We're calling this *collision avoidance*. \n",
    "\n",
    "[訳] これはとてもハードなタスクで、多くの異なるアプローチをおこないますが、多くの問題は、より簡単なサブ問題に分類する事ができます。\n",
    "最も重要で、解決すべきサブ問題は、ロボットが危険な状況に入るのを防ぐ事です。これを*collision avoidance*と呼びます。\n",
    "\n",
    "In this set of notebooks, we're going to attempt to solve the problem using deep learning and a single, very versatile, sensor: the camera.  You'll see how with a neural network, camera, and the NVIDIA Jetson Nano, we can teach the robot a very useful behavior!\n",
    "\n",
    "[訳] この章のnotebookのセットでは、単体で、非常に用途が広い、センサーであるカメラとdeep learningを用いてこの問題を解決する試みをおこないます。ニューラルネットワーク、カメラ、およびNVIDIA Jetson Nanoを使用して、ロボットに非常に有用な動作を教える方法が理解できるようになるでしょう。\n",
    "\n",
    "The approach we take to avoiding collisions is to create a virtual \"safety bubble\" around the robot.  Within this safety bubble, the robot is able to spin in a circle without hitting any objects (or other dangerous situations like falling off a ledge).  \n",
    "\n",
    "[訳] 衝突を回避するためのアプローチは、ロボットの周りに仮想的な\"safety bubble\"を作り出す事でおこないます。\"safety bubble\"の中では、ロボットは、オブジェクトにぶつかることなく（または棚から落ちるなどのその他の危険な状況にならずに）円を描くように回転できます。\n",
    "\n",
    "Of course, the robot is limited by what's in it's field of vision, and we can't prevent objects from being placed behind the robot, etc.  But we can prevent the robot from entering these scenarios itself.\n",
    "\n",
    "[訳] もちろん、もちろん、ロボットはその視野内にあるものによって制限されており、ロボットの背後などにオブジェクトが置かれる状態から回避する事はできません。しかし、ロボットがこれらのシナリオ自体に入るのを防ぐことはできます。\n",
    "\n",
    "The way we'll do this is super simple:  \n",
    "\n",
    "[訳] この方法は、実際やってみるととてもシンプルです。\n",
    "\n",
    "First, we'll manually place the robot in scenarios where it's \"safety bubble\" is violated, and label these scenarios ``blocked``.  We save a snapshot of what the robot sees along with this label.\n",
    "\n",
    "[訳] 最初に、\"safety bubble\"に違反する場所のシナリオにロボットを手動で移動します。そして、``blocked``のラベルをつけます。ラベルとともに、ロボットの見ている画像もsnapshotとして保存します。\n",
    "\n",
    "Second, we'll manually place the robot in scenarios where it's safe to move forward a bit, and label these scenarios ``free``.  Likewise, we save a snapshot along with this label.\n",
    "\n",
    "[訳] 次に、Robotをちょっと動かし安全な場所のシナリオに手動で移動します。そして、``free``のラベルをつけます。同様に、ラベルと一緒にsnapshotを保存します。　\n",
    "\n",
    "That's all that we'll do in this notebook; data collection.  Once we have lots of images and labels, we'll upload this data to a GPU enabled machine where we'll *train* a neural network to predict whether the robot's safety bubble is being violated based off of the image it sees.  We'll use this to implement a simple collision avoidance behavior in the end :)\n",
    "\n",
    "[訳] このnotebook、data collectionでは、これらをすべておこないます。ラベルと画像をたくさん用意できれば、表示される画像に基づいてロボットの安全バブルが侵害されているかどうかを予測するために、neural networkで*学習*するためのGPUマシンにデータをアップします。(追記: JetBotではJetBotのGPUを用いてそれ自身で学習可能です。)\n",
    "\n",
    "> IMPORTANT NOTE:  When JetBot spins in place, it actually spins about the center between the two wheels, not the center of the robot chassis itself.  This is an important detail to remember when you're trying to estimate whether the robot's safety bubble is violated or not.  But don't worry, you don't have to be exact. If in doubt it's better to lean on the cautious side (a big safety bubble).  We want to make sure JetBot doesn't enter a scenario that it couldn't get out of by turning in place.\n",
    "\n",
    "[訳] 重要なメモ: Jetbotが空間で回転した場合、ロボット筐体の中心ではなく、2つのWheelの中心で回転します。これは、ロボットの安全バブルが侵害されているかどうかを推定する場合に、覚えておくべき重要事項です。正確である必要はないので、心配はしないでください。疑わしい場合は、慎重な方（より大きな安全バブル）に寄りかかったほうがよい。JetBotが所定の場所に曲がって抜け出せないシナリオに入らないように注意します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display live camera feed(ライブカメラフィードを表示)\n",
    "\n",
    "So let's get started.  First, let's initialize and display our camera like we did in the *teleoperation* notebook.  \n",
    "\n",
    "(訳) それでは始めましょう。最初に、カメラの初期化と表示をおこないます。\n",
    "\n",
    "> Our neural network takes a 224x224 pixel image as input.  We'll set our camera to that size to minimize the filesize of our dataset (we've tested that it works for this task).\n",
    "> In some scenarios it may be better to collect data in a larger image size and downscale to the desired size later.\n",
    "\n",
    "> (訳) nueral networkでは、インプットとして224x224ピクセルのイメージを使います。データセットのファイルサイズを最小化するために、カメラをそのサイズに設定します(このタスクのために動作する動作する事は確認しています)\n",
    "> いくつかのシナリオでは、画像サイズを大きくし収集し、後で目的のサイズに縮小する方がいいかもしれません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)  # this width and height doesn't necessarily have to match the camera\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, next let's create a few directories where we'll store all our data.  We'll create a folder ``dataset`` that will contain two sub-folders ``free`` and ``blocked``, \n",
    "where we'll place the images for each scenario.\n",
    "\n",
    "(訳) 素晴らしい。次は、データを保存するためのディレクトリを作成しましょう。2つのサブフォルダ``free``と``block``を持つ、``dataset``フォルダを作成します。ここに、それぞれのシナリオ用の画像を置いていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blocked_dir = 'dataset/blocked'\n",
    "free_dir = 'dataset/free'\n",
    "\n",
    "# we have this \"try/except\" statement because these next functions can throw an error if the directories exist already\n",
    "try:\n",
    "    os.makedirs(free_dir)\n",
    "    os.makedirs(blocked_dir)\n",
    "except FileExistsError:\n",
    "    print('Directories not created becasue they already exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you refresh the Jupyter file browser on the left, you should now see those directories appear.  Next, let's create and display some buttons that we'll use to save snapshots\n",
    "for each class label.  We'll also add some text boxes that will display how many images of each category that we've collected so far. This is useful because we want to make\n",
    "sure we collect about as many ``free`` images as ``blocked`` images.  It also helps to know how many images we've collected overall.\n",
    "\n",
    "(訳) もし、Jupyterの左側のファイルブラウザをリフレッシュすれば、これらのディレクトリが、新規で生成された事がわかります。次に、それぞれのクラスのラベルに対応したsnapshotを保存するためのボタンを作成し、表示します。また、これまでに収集した各カテゴリの画像の数を表示するテキストボックスを追加します。``blocked``イメージの数と同じ数の``free``イメージを収集するために、これは役にたちます。また、全体で収集した画像の数を知ることも役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button_layout = widgets.Layout(width='128px', height='64px')\n",
    "free_button = widgets.Button(description='add free', button_style='success', layout=button_layout)\n",
    "blocked_button = widgets.Button(description='add blocked', button_style='danger', layout=button_layout)\n",
    "free_count = widgets.IntText(layout=button_layout, value=len(os.listdir(free_dir)))\n",
    "blocked_count = widgets.IntText(layout=button_layout, value=len(os.listdir(blocked_dir)))\n",
    "\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, these buttons wont do anything.  We have to attach functions to save images for each category to the buttons' ``on_click`` event.  We'll save the value\n",
    "of the ``Image`` widget (rather than the camera), because it's already in compressed JPEG format!\n",
    "\n",
    "(訳) 今の段階では、これらのボタンは何も反応しません。ボタンの``on_clock``イベントにそれぞれのカテゴリのためのイメージを保存する関数を対応づけます。`` Image``ウィジェット（カメラではなく）は、既に圧縮されたJPEG形式ですので、値を保存します。\n",
    "\n",
    "To make sure we don't repeat any file names (even across different machines!) we'll use the ``uuid`` package in python, which defines the ``uuid1`` method to generate\n",
    "a unique identifier.  This unique identifier is generated from information like the current time and the machine address.\n",
    "\n",
    "(訳) (異なるマシン間でも) 同じファイル名にならないうように気をつけます。ユニークな識別子を定義するために、Pythonの``uuid``パッケージを使います。このユニークな識別子は、現在時刻とマシンなどの情報から生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid1\n",
    "\n",
    "def save_snapshot(directory):\n",
    "    image_path = os.path.join(directory, str(uuid1()) + '.jpg')\n",
    "    with open(image_path, 'wb') as f:\n",
    "        f.write(image.value)\n",
    "\n",
    "def save_free():\n",
    "    global free_dir, free_count\n",
    "    save_snapshot(free_dir)\n",
    "    free_count.value = len(os.listdir(free_dir))\n",
    "    \n",
    "def save_blocked():\n",
    "    global blocked_dir, blocked_count\n",
    "    save_snapshot(blocked_dir)\n",
    "    blocked_count.value = len(os.listdir(blocked_dir))\n",
    "    \n",
    "# attach the callbacks, we use a 'lambda' function to ignore the\n",
    "# parameter that the on_click event would provide to our function\n",
    "# because we don't need it.\n",
    "free_button.on_click(lambda x: save_free())\n",
    "blocked_button.on_click(lambda x: save_blocked())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the buttons above should save images to the ``free`` and ``blocked`` directories.  You can use the Jupyter Lab file browser to view these files!\n",
    "\n",
    "(訳) すばらしい、これでボタンを押せば``free``と``blocked``ディレクトリに画像が保存されるでしょう。これらのファイルを見るには、Jupyter Labのファイルブラウザを使います。\n",
    "\n",
    "Now go ahead and collect some data \n",
    "\n",
    "1. Place the robot in a scenario where it's blocked and press ``add blocked``\n",
    "2. Place the robot in a scenario where it's free and press ``add free``\n",
    "3. Repeat 1, 2\n",
    "\n",
    "(訳) さぁ、データを集めましょう\n",
    "\n",
    "1. blockedの場所のシナリオにロボットを起き``add blocked``を押します。\n",
    "2. freeの場所のシナリオにロボットを起き``add free``を押します。\n",
    "3. 1, 2の作業を繰り返します。\n",
    "\n",
    "> REMINDER: You can move the widgets to new windows by right clicking the cell and clicking ``Create New View for Output``.  Or, you can just re-display them\n",
    "> together as we will below\n",
    "\n",
    "(訳) 覚えておいてください: セルの上で、右クリックを教えて``Create New View for Output``を選択しwidgetを新しいwindowに移動することができます。もしくは、下記のように一緒に再表示する事も可能です。\n",
    "\n",
    "Here are some tips for labeling data\n",
    "\n",
    "(訳) ラベル付けのためにいくつかのTipsがあります。\n",
    "\n",
    "1. Try different orientations\n",
    "2. Try different lighting\n",
    "3. Try varied object / collision types; walls, ledges, objects\n",
    "4. Try different textured floors / objects;  patterned, smooth, glass, etc.\n",
    "\n",
    "(訳) 1. 違う操作の試行\n",
    "2. 違う証明での試行\n",
    "3. さまざまなオブジェクト/衝突タイプ、壁、棚、オブジェクトでの試行\n",
    "4. 異なる床の模様やパターン、なめらかさや、ガラスなどでの試行\n",
    "\n",
    "Ultimately, the more data we have of scenarios the robot will encounter in the real world, the better our collision avoidance behavior will be.  It's important\n",
    "to get *varied* data (as described by the above tips) and not just a lot of data, but you'll probably need at least 100 images of each class (that's not a science, just a helpful tip here).  But don't worry, it goes pretty fast once you get going :)\n",
    "\n",
    "(訳) 最終的に、ロボットが現実の世界で遭遇するシナリオのデータが多いほど、衝突回避の挙動は正確になります。\n",
    "大量のデータだけでなく、（上記のヒントで説明したような）*さまざまな*データを取得する事が重要で、各クラスの画像が少なくとも100枚以上、必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(image)\n",
    "display(widgets.HBox([free_count, free_button]))\n",
    "display(widgets.HBox([blocked_count, blocked_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next(次)\n",
    "\n",
    "この作業は、Jetson Nano上での学習する場合は必要がないので、スキップする。\n",
    "\n",
    "Once you've collected enough data, we'll need to copy that data to our GPU desktop or cloud machine for training.  First, we can call the following *terminal* command to compress\n",
    "our dataset folder into a single *zip* file.\n",
    "\n",
    "> The ! prefix indicates that we want to run the cell as a *shell* (or *terminal*) command.\n",
    "\n",
    "> The -r flag in the zip command below indicates *recursive* so that we include all nested files, the -q flag indicates *quiet* so that the zip command doesn't print any output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r -q dataset.zip dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a file named ``dataset.zip`` in the Jupyter Lab file browser.  You should download the zip file using the Jupyter Lab file browser by right clicking and selecting ``Download``.\n",
    "\n",
    "Next, we'll need to upload this data to our GPU desktop or cloud machine (we refer to this as the *host*) to train the collision avoidance neural network.  We'll assume that you've set up your training\n",
    "machine as described in the JetBot WiKi.  If you have, you can navigate to ``http://<host_ip_address>:8888`` to open up the Jupyter Lab environment running on the host.  The notebook you'll need to open there is called ``collision_avoidance/train_model.ipynb``.\n",
    "\n",
    "So head on over to your training machine and follow the instructions there!  Once your model is trained, we'll return to the robot Jupyter Lab enivornment to use the model for a live demo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
