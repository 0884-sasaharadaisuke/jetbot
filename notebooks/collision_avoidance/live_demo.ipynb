{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance - Live Demo(デモ)\n",
    "\n",
    "In this notebook we'll use the model we trained to detect whether the robot is ``free`` or ``blocked`` to enable a collision avoidance behavior on the robot.  \n",
    "\n",
    "(訳) このnotebookでは、ロボットが「フリー」か「ブロック」かを検出するトレーニング済みモデルを使用して、ロボットの衝突回避動作を有効にします。\n",
    "\n",
    "## Load the trained model(学習済みモデルの読み込み)\n",
    "\n",
    "We'll assumed that you've already downloaded the ``best_model.pth`` to your workstation as instructed in the training notebook.  Now, you should upload this model into this notebook's\n",
    "directory by using the Jupyter Lab upload tool.  Once that's finished there should be a file named ``best_model.pth`` in this notebook's directory.  \n",
    "\n",
    "(追加) ローカルの学習なので、すでに``best_model.pth``がローカルに存在している事を前提とします。\n",
    "\n",
    "> Please make sure the file has uploaded fully before calling the next cell\n",
    "\n",
    "Execute the code below to initialize the PyTorch model.  This should look very familiar from the training notebook.\n",
    "\n",
    "(訳) Pytorch modelの初期化を下記コードでおこないます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.alexnet(pretrained=False)\n",
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_model.pth`` file that you uploaded\n",
    "\n",
    "(訳) 次に、``best_model.pth``から、学習した重みを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device.\n",
    "\n",
    "(訳) 現在、モデルの重みは、CPUメモリーで実行されているので、下記コードでGPUに転送します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessing function(事前処理関数の作成)\n",
    "\n",
    "We have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, \n",
    "we need to do some *preprocessing*.  This involves the following steps\n",
    "\n",
    "(訳) モデルを読み込みましたが、まだわずかな問題があります。学習済みモデルのフォーマットと、カメラの形式と*完全に*一致しません。これを解消するために、\n",
    "いくつかの*前処理*を行う必要があります。これらは、下記の手順になります。\n",
    "\n",
    "1. Convert from BGR to RGB\n",
    "2. Convert from HWC layout to CHW layout\n",
    "3. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "4. Transfer the data from CPU memory to GPU memory\n",
    "5. Add a batch dimension\n",
    "\n",
    "(訳)\n",
    "1. BGRからRGBに変換\n",
    "2. HWC layoutからCHW layoutに変換\n",
    "3. トレーニング中に使ったのと同じパラメーターを使用して正規化します（カメラは[0、255]の範囲の値を提供し、ロードされた画像は[0、1]の範囲でトレーニングするため、255.0でスケーリングする必要があります\n",
    "4. dataをCPUメモリからGPUメモリに転送します\n",
    "5. バッチディメンションを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "(訳) すばらしい、これでカメラ形式からneural networkのインプットのフォーマットに変換するための、pre-processing関数を定義する事ができました。\n",
    "\n",
    "Now, let's start and display our camera.  You should be pretty familiar with this by now.  We'll also create a slider that will display the\n",
    "probability that the robot is blocked.\n",
    "\n",
    "(訳) カメラを起動し、画面に表示します。カメラの起動には、大分なれてきましたね。また、ロボットがブロックされる確率が表示されるスライダーを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors.\n",
    "\n",
    "(訳) モーターを制御するためにrobotインスタンスを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes.  This function will do the following steps\n",
    "\n",
    "(訳) 次に、カメラの値が変更されるたびに呼び出される関数を作成します。この関数は下記手順で実行します。\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. While the neural network output indicates we're blocked, we'll turn left, otherwise we go forward.\n",
    "\n",
    "(訳) \n",
    "1. カメラのイメージのPre-process\n",
    "2. neural networkの実行\n",
    "3. ニューラルネットワークの出力がブロックを示した場合は、左に曲がります。それ以外の場合は前進します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "def update(change):\n",
    "    global blocked_slider, robot\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "    \n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    \n",
    "    blocked_slider.value = prob_blocked\n",
    "    \n",
    "    if prob_blocked < 0.5:\n",
    "        robot.forward(0.4)\n",
    "    else:\n",
    "        robot.left(0.4)\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "        \n",
    "update({'new': camera.value})  # we call the function once to intialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing. \n",
    "\n",
    "(訳)すばらしい、neural networkの実行関数が生成できたので、あとは処理をカメラと関連づける必要がある。\n",
    "\n",
    "We accomplish that with the ``observe`` function.\n",
    "\n",
    "(訳) それは `` observe``関数で実現します。\n",
    "\n",
    "> WARNING: This code will move the robot!! Please make sure your robot has clearance.  The collision avoidance should work, but the neural\n",
    "> network is only as good as the data it's trained on!\n",
    "\n",
    "> (訳)\n",
    "> 注意: このコードでロボットが動きだします。十分なスペースを確保してください。collision avoidanceが動くでしょう、neral networkは、訓練されたデータと同じくらいに動くはずです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame.  Perhaps start by placing your robot on the ground and seeing what it does when it reaches an obstacle.\n",
    "\n",
    "(訳) 驚くばかり！ロボットが接続されている場合、新しいカメラフレームごとに新しいコマンドが生成されるはずです。おそらく、ロボットを地面に置き、障害物に到達したときの動作を確認する事ができるでしょう。\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below.\n",
    "\n",
    "(訳)もし、今の挙動を止めたい場合は、下記コードを実行し、このカールバックの関連付けをやめます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "camera.unobserve(update, names='value')\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want the robot to run without streaming video to the browser.  You can unlink the camera as below.\n",
    "\n",
    "(訳) もしかしたら、videoをブラウザに転送しないでロボットを実行したいと思いますが、その場合は、下記のようにcameraをunkiklｋしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.unlink()  # don't stream to browser (will still run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue streaming call the following.\n",
    "\n",
    "(訳) streamingを続ける場合は、下記コードで続けます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.link()  # stream to browser (wont run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "That's it for this live demo!  Hopefully you had some fun and your robot avoided collisions intelligently! \n",
    "\n",
    "(訳) 以上がLive demoです。うまく行けば、ロボットが衝突をインテリジェントに回避できたことと思います。\n",
    "\n",
    "If your robot wasn't avoiding collisions very well, try to spot where it fails.  The beauty is that we can collect more data for these failure scenari\n",
    "and the robot should get even better :)\n",
    "\n",
    "(訳) avoiding collisionsが上手く行かない場合、失敗した箇所を見つけてください。美しさは、これら上手くいかないシナリオについてより多くのデータを収集すれば、ロボットはさらに良くなるはずです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
