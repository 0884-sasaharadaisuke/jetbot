{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odWX-mH4PoHP"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1FphScAZPoHW"
   },
   "outputs": [],
   "source": [
    "# IPython Libraries for display and widgets\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Camera and Motor Interface for JetBot\n",
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "\n",
    "# Python basic pakcages for image annotation\n",
    "from uuid import uuid1\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjKffuTgPoHl"
   },
   "source": [
    "### Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3716sYvxPoHp"
   },
   "outputs": [],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "\n",
    "# CSIカメラの設定、幅と高さも設定している\n",
    "camera = CSICamera(width=224, height=224)\n",
    "\n",
    "# カメラを動作させる設定\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5dZrEmFSPoHz"
   },
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NrKMvR09PoH1"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from xy_dataset import XYDataset\n",
    "\n",
    "TASK = 'road_following'\n",
    "\n",
    "CATEGORIES = ['apex']\n",
    "\n",
    "DATASETS = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "\n",
    "# データAugmentataionの設定,ColorJitterはに対して明るさ、コントラスト、彩度（鮮やかさ）、色相を設定、 NormalizeはRGBに対して平均と分散で正規化を行っている\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = XYDataset(TASK + '_' + name, CATEGORIES, TRANSFORMS, random_hflip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aQ5eur-0PoH8"
   },
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92AXO0jhPoH-",
    "outputId": "dfb66de5-0783-4ef2-eb01-676bca026336"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2dad71109d246c4a5d57c68d2821d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ClickableImageWidget(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "\n",
    "# 設定したいカテゴリのデータセットを指定\n",
    "dataset = datasets[DATASETS[1]]\n",
    "\n",
    "# このセルの2度目の実行に備えてカメラからの観測をオフにする\n",
    "camera.unobserve_all()\n",
    "\n",
    "# イメージのプレビューを作成\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# データセット、カテゴリ、データの数を把握できるようにwidgetを作成\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "\n",
    "# 現在のデータセットの数を取得して更新\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# データセットを設定\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# 新しいカテゴリのデータセットになったときのデータセットの数を更新\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "# 画像をクリックしてデータを保存。データセット名にx,y座標を加えて保存する。保存したデータを表示してカウントを更新する\n",
    "def save_snapshot(_, content, msg):\n",
    "    if content['event'] == 'click':\n",
    "        data = content['eventData']\n",
    "        x = data['offsetX']\n",
    "        y = data['offsetY']\n",
    "        \n",
    "        # ディスクにデータを保存する\n",
    "        dataset.save_entry(category_widget.value, camera.value, x, y)\n",
    "        \n",
    "        # 保存したデータを表示\n",
    "        snapshot = camera.value.copy()\n",
    "        snapshot = cv2.circle(snapshot, (x, y), 8, (0, 255, 0), 3)\n",
    "        snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "        count_widget.value = dataset.get_count(category_widget.value)\n",
    "        \n",
    "camera_widget.on_msg(save_snapshot)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    dataset_widget,\n",
    "    category_widget,\n",
    "    count_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWkHaDhnPoIF"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iX316hHsPoIH",
    "outputId": "607ddb54-ee92-40e7-f1dc-4295fbee9c6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e2bf462d4f492da7b0c19ecdc29bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='road_following_model_A.pth', description='model path'), HBox(children=(Button(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# GPUで学習するための設定\n",
    "device = torch.device('cuda')\n",
    "# x,y座標 * カテゴリ数\n",
    "output_dim = 2 * len(dataset.categories)\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, output_dim)\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, output_dim, kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, output_dim)\n",
    "\n",
    "# DENSENET 121\n",
    "# model = torchvision.models.densenet121(pretrained=True)\n",
    "# model.classifier = torch.nn.Linear(model.num_features, output_dim)\n",
    "\n",
    "# GPU か CPUの設定\n",
    "model = model.to(device)\n",
    "\n",
    "# モデルのパスの設定と保存、ロードするためのボタンを表示\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='road_following_model_A.pth')\n",
    "\n",
    "# pytorchのモデルをロードするための関数\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "\n",
    "# モデルをロードする機能を追加\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "# モデルを保存する関数\n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "\n",
    "# モデルを保存する機能を追加\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "# ボタンを設定\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wk8q38cpPoIN"
   },
   "source": [
    "### Live Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKBFXVPJPoIP",
    "outputId": "390a4724-4780-4b67-d090-a77f464a5715"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c107794149424d90d9b2ccfc579f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'', format='jpeg', height='224', width='224'), ToggleButtons(description='state', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# カメラ機能の状態を設定するためのボタン\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "# 予測の様子を表示するためのWidget\n",
    "prediction_widget = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "\n",
    "# 予測の状況をライブする関数\n",
    "def live(state_widget, model, camera, prediction_widget):\n",
    "    global dataset\n",
    "    # live状態であると動作\n",
    "    while state_widget.value == 'live':\n",
    "        # カメラからデータを取得\n",
    "        image = camera.value\n",
    "        # 前処理を行う\n",
    "        preprocessed = preprocess(image)\n",
    "        # 予測を行い、予測値をCPUで処理する形に変更\n",
    "        output = model(preprocessed).detach().cpu().numpy().flatten()\n",
    "        # 設定したカテゴリ値を取得\n",
    "        category_index = dataset.categories.index(category_widget.value)\n",
    "        # x, y 座標に変換\n",
    "        x = output[2 * category_index]\n",
    "        y = output[2 * category_index + 1]\n",
    "        \n",
    "        # カメラの座標に変換。中央からの値にしたいため、2で割って0.5を足している\n",
    "        x = int(camera.width * (x / 2.0 + 0.5))\n",
    "        y = int(camera.height * (y / 2.0 + 0.5))\n",
    "        \n",
    "        # カメラ画像をコピー\n",
    "        prediction = image.copy()\n",
    "        # カメラの予測値をサークルとして追加\n",
    "        prediction = cv2.circle(prediction, (x, y), 8, (255, 0, 0), 3)\n",
    "        # カメラの予測値をjpeg値に変換してwidgetに追加\n",
    "        prediction_widget.value = bgr8_to_jpeg(prediction)\n",
    "\n",
    "# カメラの状態を確認してカメラを起動\n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "# カメラ機能の状態を確認してカメラを起動するかどうかを設定\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "# カメラの状態を設定するボタンと予測の状態を表示する画面を設定\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    prediction_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHT5FHB0PoIT"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "# RGBを正規化するための平均と分散を設定\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "\n",
    "def preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    # FP16の精度でPyTorchで扱えるデータ形式（Tensor）で変換し、さらにGPUで予測可能なデータに変換\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    # 平均と分散で正規化\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nV1YzT98PoIU",
    "outputId": "63ef690a-d2b7-4203-f895-ae7be32a4b30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7430619a3ec141a99b329e041102fe82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Epoch, 評価、学習、lossをボタンとして表示、学習の進行状況を確認するプログレスバーを追加\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "# 学習用のデータセットを指定。indexが0の場合はAのデータセット, indexが1の場合はBのデータセット, \n",
    "dataset = datasets[DATASETS[1]]\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        # 学習データをローダーとして設定\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # 学習中にボタン操作しないための設定\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        # モデルの学習と評価で動作を振り分け\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "\n",
    "        # Epochの間、学習を行う\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            # バッチサイズごとに学習データ、カテゴリ、xy座標を取得\n",
    "            for images, category_idx, xy in iter(train_loader):\n",
    "                # GPU用にデータを変換\n",
    "                images = images.to(device)\n",
    "                preprocessed = preprocess(images)\n",
    "                xy = xy.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # 勾配を初期化して前回の勾配を使用しないように\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # モデルの出力を取得\n",
    "                outputs = model(images)\n",
    "\n",
    "                # カテゴリに応じたx,y座標に対してMSEロスを計算\n",
    "                loss = 0.0\n",
    "                for batch_idx, cat_idx in enumerate(list(category_idx.flatten())):\n",
    "                    loss += torch.mean((outputs[batch_idx][2 * cat_idx:2 * cat_idx+2] - xy[batch_idx])**2)\n",
    "                loss /= len(category_idx)\n",
    "\n",
    "                if is_training:\n",
    "                    # 誤差逆伝番を実行\n",
    "                    loss.backward()\n",
    "\n",
    "                    # パラメータを調整\n",
    "                    optimizer.step()\n",
    "\n",
    "                # プログレスバーの更新\n",
    "                count = len(category_idx.flatten())\n",
    "                i += count\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                # トータルロスの更新\n",
    "                sum_loss += float(loss)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                \n",
    "            # 学習時はepochを現象させる。評価時は1回のみの実行で終了\n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        print(\"Error {}\".format(e))\n",
    "        pass\n",
    "    # モデルが評価用になる。Batch Normが学習時の平均、分散を使用。Drop Outは働かなくなる\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "\n",
    "# 学習と評価用のボタンを追加\n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "\n",
    "# epoch, プログレスバー、ロス、学習、評価用ボタンをまとめて扱う\n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "# Widgetを表示\n",
    "display(train_eval_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-uZzdHvXA9v"
   },
   "source": [
    "# TensorRT Convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rWOpRLEPoIf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "CATEGORIES = ['apex']\n",
    "\n",
    "# # GPU用にデバイスを設定\n",
    "# device = torch.device('cuda')\n",
    "# # モデルの型だけを取得\n",
    "# model = torchvision.models.resnet18(pretrained=False)\n",
    "# # モデルの最終層を変更（カテゴリ*xy座標）\n",
    "# model.fc = torch.nn.Linear(512, 2 * len(CATEGORIES))\n",
    "# # モデルをGPUの評価用に変更\n",
    "# model = model.cuda().eval().half()\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g83MsfiNPoIh"
   },
   "source": [
    "Next, load the saved model.  Enter the model path you used to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Jbjy_h0PoIi"
   },
   "outputs": [],
   "source": [
    "# モデルをロードして先ほど設定したモデルの型に重みを設定\n",
    "# model.load_state_dict(torch.load('road_following_model2.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_gGnYMV0PoIl"
   },
   "source": [
    "Convert and optimize the model using ``torch2trt`` for faster inference with TensorRT.  Please see the [torch2trt](https://github.com/NVIDIA-AI-IOT/torch2trt) readme for more details.\n",
    "\n",
    "> This optimization process can take a couple minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw_emR96PoIm",
    "outputId": "2129d6da-404f-41e8-a18c-ab4b72dfe826"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 6.74 s, total: 19.4 s\n",
      "Wall time: 37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch2trt import torch2trt\n",
    "\n",
    "# 空のデータをTensorRT変換用に準備\n",
    "# TensorRT\n",
    "#    https://developer.nvidia.com/tensorrt\n",
    "data = torch.zeros((1, 3, 224, 224)).cuda()\n",
    "\n",
    "# モデルをFP16でTensorRT用に変更\n",
    "model_trt = torch2trt(model, [data], fp16_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRwsisisPoIq"
   },
   "source": [
    "Save the optimized model using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDLidxQ_PoIr"
   },
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "torch.save(model_trt.state_dict(), 'road_following_model_B2_trt.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "interactive_regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
